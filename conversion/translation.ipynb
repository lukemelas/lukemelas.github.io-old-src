{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Translation.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Lrkez3VtH1Wf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Neural Machine Translation in PyTorch: Build your own mini Google Translate! \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "nvm45PzvmczH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Introduction"
      ]
    },
    {
      "metadata": {
        "id": "MPNK98Emmf9j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this post, we're going to build a machine learning model to translate German to English. It's not quite going to be Google Translate, but it will actually come surprisingly close! \n",
        "\n",
        "We'll be using the same model architecture as Google, an sequence-to-sequence model, with a few elements removed. For the details on Google's setup, you can read [their paper](https://arxiv.org/abs/1609.08144) (Wu et al.). For our setup, we'll work primarily from [this paper](https://arxiv.org/abs/1409.0473) (Bahdanau et al.), a breakthough paper that served as the base of the Google model. \n",
        "\n",
        "We will build the model from scratch with PyTorch 0.4 and Python 3. In fact, everything here is an iPython notebook (published [here](http://www.example.com)) that you can run yourself. Additionally, everything is uploaded publicly to [GitHub](https://github.com/lukemelas/Machine-Translation).  \n",
        "\n",
        "At the end of the day, you'll be able to produce translations like these: "
      ]
    },
    {
      "metadata": {
        "id": "uKtkBhL_oZsk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "|     |                               |                                                                   |\n",
        "| :-- | :---------------------------- | :---------------------------------------------------------------- |\n",
        "|     | **German**                    | Dieser Tag hat unsere Sicht nachhaltig ver√§ndert .                |\n",
        "|     | **Professional Translation**  | And that day really changed our perspective .                     |\n",
        "|     | **Our Translation**           | This day, our view has changed .                                  |\n",
        "|     |                               |                                                                   |\n",
        "\n",
        "|     |                               |                                                                   |\n",
        "| :-- | :---------------------------- | :---------------------------------------------------------------- |\n",
        "|     | **German**                    | So lernt man als Kind eine Sprache .                              |\n",
        "|     | **Professional Translation**  | And this is what you learn when you learn a language as a child . |\n",
        "|     | **Our Translation**           | This is how you learn a language as a child .                     |\n",
        "|     |                               |                                                                   |\n",
        "\n",
        "|     |                               |                                                                   |\n",
        "| :-- | :---------------------------- | :---------------------------------------------------------------- |\n",
        "|     | **German**                    | Man kann eine Kultur ohne Austausch pflegen .                     |\n",
        "|     | **Professional Translation**  | You can have culture without exchange .                           |\n",
        "|     | **Our Translation**           | You can exchange a culture without exchange .                     |"
      ]
    },
    {
      "metadata": {
        "id": "fUPM8tNQnvxI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "This tutorial is ideally for someone with some experience with neural networks or recurrent neural networks, but perhaps without much experience in translation or natural language processing. \n",
        "\n",
        "For those looking to take machine translation to the next level, try out the brilliant [OpenNMT](https://github.com/OpenNMT/OpenNMT-py) platform, also built in PyTorch. \n",
        "\n",
        "Now, let's dive into translation. "
      ]
    },
    {
      "metadata": {
        "id": "jqLZaL9s6peb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Overview \n",
        "\n",
        "Our goal is to convert a German source sentence into an English sentence. To do this, we will first *encode* each word of the German sentence and then *decode* an English sentence one word at a time. During decoding, we will use *attention* to look at the encoded English words as we go along. \n",
        "\n",
        "For example, when trying to translate the third sentence above, in the first step of decoding, we might *attend* to the encoding of German word \"Man\" in the source sentence and then produce the English word \"You\". For this reason, we call our model an encoder-decoder sequence-to-sequence model. This type of model is now ubiquitous in natural language processing and other areas that deal with sequences (text, speech, etc.). \n",
        "\n",
        "In this case, both our encder and decoder are *recurrent neural networks,* specifically long-short term memory networks. Fortunately for us, LSTMS have already been implemented in PyTorch. \n"
      ]
    },
    {
      "metadata": {
        "id": "-8QLJ73wB_C_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### The Data\n",
        "\n",
        "To train our translation model, we're going to need a ton of German-English sentence pairs translated by professional translators. Today, we'll use a database of German TED Talks translated into English called IWSLT. For training bigger models, researchers also often use the proceedings of the European Parliament. \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "4urou0DmvVlL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We preprocess the data with the `spacy` library. In preprocessing, we split our sentence into `tokens` (words) and add special `<s>` and `</s>` tokens to mark the beginning and end of sentences.  We also replace words which occur fewer than 5 times with an unknown token `<unk>` -- this helps us keep our vocabulary to a managable `13353` words. "
      ]
    },
    {
      "metadata": {
        "id": "8ucSxpSQxDya",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If you're running this code on your own computer, this preprocessing might take a little while. "
      ]
    },
    {
      "metadata": {
        "id": "ylYdIfFHwfD-",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "collapsed": true,
        "cellView": "both"
      },
      "cell_type": "code",
      "source": [
        "## Uncomment these lines if you have not downloaded spacy and torchtext\n",
        "# !pip install spacy\n",
        "# !pip install torch torchvision torchtext\n",
        "# !python -m spacy download en\n",
        "# !python -m spacy download de"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J8DF8S_mxPBo",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import itertools, os, time , datetime\n",
        "import numpy as np\n",
        "import spacy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchtext import data, datasets\n",
        "from torchtext.vocab import Vectors, GloVe\n",
        "use_gpu = torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iGNpax-brHmz",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "collapsed": true,
        "cellView": "both",
        "outputId": "3856caa7-4852-4a2f-f3ea-b5f5cb9a4e5a",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527183559621,
          "user_tz": 240,
          "elapsed": 224403,
          "user": {
            "displayName": "Luke Melas-Kyriazi",
            "photoUrl": "//lh4.googleusercontent.com/-sqV5tJyQuHE/AAAAAAAAAAI/AAAAAAAANzo/VjDqA2ilBo4/s50-c-k-no/photo.jpg",
            "userId": "103761958259602667321"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def preprocess(vocab_size=0, batchsize=16, max_sent_len=20):\n",
        "    '''Loads data from text files into iterators'''\n",
        "\n",
        "    # Load text tokenizers\n",
        "    spacy_de = spacy.load('de')\n",
        "    spacy_en = spacy.load('en')\n",
        "\n",
        "    def tokenize(text, lang='en'):\n",
        "        if lang is 'de':\n",
        "            return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "        elif lang is 'en':\n",
        "            return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "        else:\n",
        "            raise Exception('Invalid language')\n",
        "\n",
        "    # Add beginning-of-sentence and end-of-sentence tokens \n",
        "    BOS_WORD = '<s>'\n",
        "    EOS_WORD = '</s>'\n",
        "    DE = data.Field(tokenize=lambda x: tokenize(x, 'de'))\n",
        "    EN = data.Field(tokenize=tokenize, init_token=BOS_WORD, eos_token=EOS_WORD)\n",
        "\n",
        "    # Create sentence pair dataset with max length 20\n",
        "    train, val, test = datasets.IWSLT.splits(exts=('.de', '.en'), fields=(DE, EN), filter_pred = lambda x: max(len(vars(x)['src']), len(vars(x)['trg'])) <= max_sent_len)\n",
        "\n",
        "    # Build vocabulary and convert text to indices\n",
        "    # Convert words that appear fewer than 5 times to <unk>\n",
        "    if vocab_size > 0:\n",
        "        DE.build_vocab(train.src, min_freq=5, max_size=vocab_size)\n",
        "        EN.build_vocab(train.trg, min_freq=5, max_size=vocab_size)\n",
        "    else:\n",
        "        DE.build_vocab(train.src, min_freq=5)\n",
        "        EN.build_vocab(train.trg, min_freq=5)\n",
        "\n",
        "    # Create iterators to process text in batches of approx. the same length\n",
        "    train_iter = data.BucketIterator(train, batch_size=batchsize, device=-1, repeat=False, sort_key=lambda x: len(x.src))\n",
        "    val_iter = data.BucketIterator(val, batch_size=1, device=-1, repeat=False, sort_key=lambda x: len(x.src))\n",
        "    \n",
        "    return DE, EN, train_iter, val_iter\n",
        "\n",
        "# Test\n",
        "timer = time.time()\n",
        "SRC, TGT, train_iter, val_iter = preprocess()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading de-en.tgz\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.tst2014.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.dev2012.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2014.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2012.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2010.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.dev2010.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2010.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2011.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.tst2013.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.dev2012.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2013.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2013.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.tst2014.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.tst2013.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.dev2010.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2012.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2011.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2014.de-en.en.xml\n",
            ".data/iwslt/de-en/train.tags.de-en.de\n",
            ".data/iwslt/de-en/train.tags.de-en.en\n",
            "This is a test of our preprocessing function. It took 223.9 seconds to load the data. \n",
            "         Our German vocab has size 13353 and our English vocab has size 11560.\n",
            "         Our training data is 7443 batches of 16 sentences and our validation data is 570 batches.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DdDVU9IVRXj3",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "390513a4-8dda-4d4e-eb6a-c7b57201bb21",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527184293933,
          "user_tz": 240,
          "elapsed": 565,
          "user": {
            "displayName": "Luke Melas-Kyriazi",
            "photoUrl": "//lh4.googleusercontent.com/-sqV5tJyQuHE/AAAAAAAAAAI/AAAAAAAANzo/VjDqA2ilBo4/s50-c-k-no/photo.jpg",
            "userId": "103761958259602667321"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print('''This is a test of our preprocessing function. It took {:.1f} seconds to load the data. \n",
        "Our German vocab has size {} and our English vocab has size {}.\n",
        "Our training data has {} batches, each with {} sentences, and our validation data has {} batches.'''.format(\n",
        "time.time() - timer, len(SRC.vocab), len(TGT.vocab), len(train_iter), train_iter.batch_size, len(val_iter)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a test of our preprocessing function. It took 202.0 seconds to load the data. \n",
            "Our German vocab has size 13353 and our English vocab has size 11560.\n",
            "Our training data has 7443 batches, each with 16 sentences, and our validation data has 570 batches.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UuXUTKG16pbT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "### The Model\n",
        "\n",
        "![Attention](https://github.com/lukemelas/lukemelas.github.io-src/raw/master/content/assets/images/translation/translation_diagram.png)\n",
        "\n",
        "#### Word Embeddings\n",
        "\n",
        "The first step of both the encoder and decoder is to convert the input words to into vectors, a form our model can work with. We do so with word embeddings, which are mappings from each word in our vocab to a vector in some high-dimensional space (say, 300 dimensions). \n",
        "\n",
        "Word embeddings are a subject for an entirely separate blog post, but the basic idea is that word vectors capture some semantic meaning: the vector for `dog` is closer to the vector for `cat` than the vector for `asparagus`.\n",
        "\n",
        "You can get your own vectors from a source like`GloVe` or `fastText`, or you can use the ones I've generated and uploaded to GitHub (links below). "
      ]
    },
    {
      "metadata": {
        "id": "PIMCrj0x4sgB",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "collapsed": true,
        "outputId": "80e7ffad-8477-4c01-b2d0-317f37c5155a",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527184299479,
          "user_tz": 240,
          "elapsed": 4160,
          "user": {
            "displayName": "Luke Melas-Kyriazi",
            "photoUrl": "//lh4.googleusercontent.com/-sqV5tJyQuHE/AAAAAAAAAAI/AAAAAAAANzo/VjDqA2ilBo4/s50-c-k-no/photo.jpg",
            "userId": "103761958259602667321"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://github.com/lukemelas/Machine-Translation/raw/master/scripts/emb-13353-de.npy https://github.com/lukemelas/Machine-Translation/raw/master/scripts/emb-11560-en.npy"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-05-24 17:51:37--  https://github.com/lukemelas/Machine-Translation/raw/master/scripts/emb-13353-de.npy\r\n",
            "Resolving github.com (github.com)... 192.30.253.112, 192.30.253.113\r\n",
            "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/lukemelas/Machine-Translation/master/scripts/emb-13353-de.npy [following]\n",
            "--2018-05-24 17:51:37--  https://raw.githubusercontent.com/lukemelas/Machine-Translation/master/scripts/emb-13353-de.npy\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 32047280 (31M) [application/octet-stream]\n",
            "Saving to: ‚Äòemb-13353-de.npy‚Äô\n",
            "\n",
            "emb-13353-de.npy    100%[===================>]  30.56M  38.6MB/s    in 0.8s    \n",
            "\n",
            "2018-05-24 17:51:38 (38.6 MB/s) - ‚Äòemb-13353-de.npy‚Äô saved [32047280/32047280]\n",
            "\n",
            "--2018-05-24 17:51:38--  https://github.com/lukemelas/Machine-Translation/raw/master/scripts/emb-11560-en.npy\n",
            "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/lukemelas/Machine-Translation/master/scripts/emb-11560-en.npy [following]\n",
            "--2018-05-24 17:51:38--  https://raw.githubusercontent.com/lukemelas/Machine-Translation/master/scripts/emb-11560-en.npy\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13872080 (13M) [application/octet-stream]\n",
            "Saving to: ‚Äòemb-11560-en.npy‚Äô\n",
            "\n",
            "emb-11560-en.npy    100%[===================>]  13.23M  22.6MB/s    in 0.6s    \n",
            "\n",
            "2018-05-24 17:51:39 (22.6 MB/s) - ‚Äòemb-11560-en.npy‚Äô saved [13872080/13872080]\n",
            "\n",
            "FINISHED --2018-05-24 17:51:39--\n",
            "Total wall clock time: 2.3s\n",
            "Downloaded: 2 files, 44M in 1.4s (31.8 MB/s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2iFfBRzL4vl9",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def load_embeddings(SRC, TGT, np_src_file, tgt_file):\n",
        "    '''Load English and German embeddings from saved numpy files'''\n",
        "    emb_tr_src = torch.from_numpy(np.load(np_src_file))\n",
        "    emb_tr_tgt = torch.from_numpy(np.load(np_tgt_file))\n",
        "    return emb_tr_src, emb_tr_tgt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ktFoVbXN4tPC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Encoder\n",
        "\n",
        "Our encoder (red in the model diagram above) is a bidirectional recurrent neural network. Bidirectional simply means that we run the model both backwards and forwards along the sentence. \n",
        "\n",
        "Our encoder outputs a vector for each word in the source sentence. All these vectors together are called a *memory bank*. "
      ]
    },
    {
      "metadata": {
        "id": "VMKaLXm44sav",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class EncoderLSTM(nn.Module):\n",
        "    def __init__(self, embedding, h_dim, num_layers, dropout_p=0.0, bidirectional=True):\n",
        "        super(EncoderLSTM, self).__init__()\n",
        "        self.vocab_size, self.embedding_size = embedding.size()\n",
        "        self.num_layers, self.h_dim, self.dropout_p, self.bidirectional = num_layers, h_dim, dropout_p, bidirectional \n",
        "\n",
        "        # Create embedding and LSTM\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
        "        self.embedding.weight.data.copy_(embedding)\n",
        "        self.lstm = nn.LSTM(self.embedding_size, self.h_dim, self.num_layers, dropout=self.dropout_p, bidirectional=bidirectional)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''Embed text, get initial LSTM hidden state, and encode with LSTM'''\n",
        "        x = self.dropout(self.embedding(x)) # embedding\n",
        "        h0 = self.init_hidden(x.size(1)) # initial state of LSTM\n",
        "        memory_bank, h = self.lstm(x, h0) # encoding\n",
        "        return memory_bank, h\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        '''Create initial hidden state of zeros: 2-tuple of num_layers x batch size x hidden dim'''\n",
        "        num_layers = self.num_layers * 2 if self.bidirectional else self.num_layers\n",
        "        init = torch.zeros(num_layers, batch_size, self.h_dim)\n",
        "        init = init.cuda() if use_gpu else init\n",
        "        h0 = (init, init.clone())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mlF3nu_y-tvo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Attention\n",
        "\n",
        "Attention enables our decoder to look at our encoded source words while translating. We use dot-product attention, which means we take the dot product of our intermediate decoder output and our encoder output. We then take a weighted sum of our encoder vectors, using this dot product as the weight. \n",
        "\n",
        "There are lots of other types of attention, described in more detail [here](http://ruder.io/deep-learning-nlp-best-practices/index.html#attention), but we use dot-product attention because it is simple and works well. "
      ]
    },
    {
      "metadata": {
        "id": "NuH-ZC616ThJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, pad_token=1, bidirectional=True, h_dim=300):\n",
        "        super(Attention, self).__init__()\n",
        "        self.bidirectional, self.h_dim, self.pad_token = bidirectional, h_dim, pad_token\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, in_e, out_e, out_d):\n",
        "        '''Produces context with attention distribution'''\n",
        "\n",
        "        # Deal with bidirectional encoder, move batches first\n",
        "        if self.bidirectional: # sum hidden states for both directions\n",
        "            out_e = out_e.contiguous().view(out_e.size(0), out_e.size(1), 2, -1).sum(2).view(out_e.size(0), out_e.size(1), -1)\n",
        "            \n",
        "        # Move batches first\n",
        "        out_e = out_e.transpose(0,1) # b x sl x hd\n",
        "        out_d = out_d.transpose(0,1) # b x tl x hd\n",
        "\n",
        "        # Dot product attention, softmax, and reshape\n",
        "        attn = out_e.bmm(out_d.transpose(1,2)) # (b x sl x hd) (b x hd x tl) --> (b x sl x tl)\n",
        "        attn = self.softmax(attn).transpose(1,2) # --> b x tl x sl\n",
        "\n",
        "        # Get attention distribution\n",
        "        context = attn.bmm(out_e) # --> b x tl x hd\n",
        "        context = context.transpose(0,1) # --> tl x b x hd\n",
        "        return context"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6XAHZebS6U7w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Decoder\n",
        "\n",
        "Our decoder (blue and yellow in the diagram) is a recurrent neural network. Within our decoder, we have an *attention* layer, which looks at the memory bank from the encoder. \n",
        "\n",
        "We begin by feeding in the start token `<s>`. Our decoder tries to predict the next word by outputting a distribution over all words in the vocabulary. During training, we know the ground truth sentence, so we feed it into the decoder word-by-word at each step. We penalize the model's predictions using a cross-entropy loss function. During testing, we do not know the ground truth, so we use a prediction of the model as input to the next time step. We'll discuss this process in more detail below.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "W8zFya8t6UwP",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class DecoderLSTM(nn.Module):\n",
        "    def __init__(self, embedding, h_dim, num_layers, dropout_p=0.0):\n",
        "        super(DecoderLSTM, self).__init__()\n",
        "        self.vocab_size, self.embedding_size = embedding.size()\n",
        "        self.num_layers, self.h_dim, self.dropout_p = num_layers, h_dim, dropout_p\n",
        "        \n",
        "        # Create embedding and LSTM\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
        "        self.embedding.weight.data.copy_(embedding) \n",
        "        self.lstm = nn.LSTM(self.embedding_size, self.h_dim, self.num_layers, dropout=self.dropout_p)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "    \n",
        "    def forward(self, x, h0):\n",
        "        '''Embed text and pass through LSTM'''\n",
        "        x = self.embedding(x)\n",
        "        x = self.dropout(x)\n",
        "        out, h = self.lstm(x, h0)\n",
        "        return out, h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E27dulp4ACe9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Beam Search\n",
        "\n",
        "At test time, we need to use the output of our decoder as the input to the model at the next time step. We could do this by taking the most likely word each time, a strategy known as greedy search. Here, we'll use a fancier method known as beam search, which keeps around a list of likely partial sentences during decoding. \n",
        "\n",
        "Beam search is a bit complicated, but I've decided to include it because few other tutorials do, and because it really does improve translation quality. To make it simpler, I've only implemented it for batch size = 1. Since beam search will be a method of our final model class, the code is in the section below. "
      ]
    },
    {
      "metadata": {
        "id": "CA9bc-9cB9oN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Final model\n",
        "\n",
        "Our final model combines the encoder, attention, decoder, and beam search. We call it *Seq2seq*."
      ]
    },
    {
      "metadata": {
        "id": "y5N93D6XB9M-",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class Seq2seq(nn.Module):\n",
        "    def __init__(self, embedding_src, embedding_tgt, h_dim, num_layers, dropout_p, bi, tokens_bos_eos_pad_unk=[0,1,2,3]):\n",
        "        super(Seq2seq, self).__init__()\n",
        "        # Store hyperparameters\n",
        "        self.h_dim = h_dim\n",
        "        self.vocab_size_tgt, self.emb_dim_tgt = embedding_tgt.size()\n",
        "        self.bos_token, self.eos_token, self.pad_token, self.unk_token = tokens_bos_eos_pad_unk\n",
        "\n",
        "        # Create encoder, decoder, attention\n",
        "        self.encoder = EncoderLSTM(embedding_src, h_dim, num_layers, dropout_p=dropout_p, bidirectional=bi)\n",
        "        self.decoder = DecoderLSTM(embedding_tgt, h_dim, num_layers * 2 if bi else num_layers, dropout_p=dropout_p)\n",
        "        self.attention = Attention(pad_token=self.pad_token, bidirectional=bi, h_dim=self.h_dim)\n",
        "\n",
        "        # Create linear layers to combine context and hidden state\n",
        "        self.linear1 = nn.Linear(2 * self.h_dim, self.emb_dim_tgt)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.linear2 = nn.Linear(self.emb_dim_tgt, self.vocab_size_tgt)\n",
        "        \n",
        "        # Share weights between decoder embedding and output \n",
        "        if self.decoder.embedding.weight.size() == self.linear2.weight.size():\n",
        "            self.linear2.weight = self.decoder.embedding.weight\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        if use_gpu: src = src.cuda()\n",
        "        \n",
        "        # Encode\n",
        "        out_e, final_e = self.encoder(src)\n",
        "        \n",
        "        # Decode\n",
        "        out_d, final_d = self.decoder(tgt, final_e)\n",
        "        \n",
        "        # Attend\n",
        "        context = self.attention(src, out_e, out_d)\n",
        "        out_cat = torch.cat((out_d, context), dim=2) \n",
        "        \n",
        "        # Predict (returns probabilities)\n",
        "        x = self.linear1(out_cat)\n",
        "        x = self.dropout(self.tanh(x))\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "    def predict(self, src, beam_size=1): \n",
        "        '''Predict top 1 sentence using beam search. Note that beam_size=1 is greedy search.'''\n",
        "        beam_outputs = self.beam_search(src, beam_size, max_len=30) # returns top beam_size options (as list of tuples)\n",
        "        top1 = beam_outputs[0][1] # a list of word indices (as ints)\n",
        "        return top1\n",
        "\n",
        "    def beam_search(self, src, beam_size, max_len, remove_tokens=[]):\n",
        "        '''Returns top beam_size sentences using beam search. Works only when src has batch size 1.'''\n",
        "        if use_gpu: src = src.cuda()\n",
        "        \n",
        "        # Encode\n",
        "        outputs_e, states = self.encoder(src) # batch size = 1\n",
        "        \n",
        "        # Start with '<s>'\n",
        "        init_lprob = -1e10\n",
        "        init_sent = [self.bos_token]\n",
        "        best_options = [(init_lprob, init_sent, states)] # beam\n",
        "        \n",
        "        # Beam search\n",
        "        k = beam_size # store best k options\n",
        "        for length in range(max_len): # maximum target length\n",
        "            options = [] # candidates \n",
        "            for lprob, sentence, current_state in best_options:\n",
        "                # Prepare last word\n",
        "                last_word = sentence[-1]\n",
        "                if last_word != self.eos_token:\n",
        "                    last_word_input = torch.LongTensor([last_word]).view(1,1)\n",
        "                    if use_gpu: last_word_input = last_word_input.cuda()\n",
        "                    # Decode\n",
        "                    outputs_d, new_state = self.decoder(last_word_input, current_state)\n",
        "                    # Attend\n",
        "                    context = self.attention(src, outputs_e, outputs_d)\n",
        "                    out_cat = torch.cat((outputs_d, context), dim=2)\n",
        "                    x = self.linear1(out_cat)\n",
        "                    x = self.dropout(self.tanh(x))\n",
        "                    x = self.linear2(x)\n",
        "                    x = x.squeeze().data.clone()\n",
        "                    # Block predictions of tokens in remove_tokens\n",
        "                    for t in remove_tokens: x[t] = -10e10\n",
        "                    lprobs = torch.log(x.exp() / x.exp().sum()) # log softmax\n",
        "                    # Add top k candidates to options list for next word\n",
        "                    for index in torch.topk(lprobs, k)[1]: \n",
        "                        option = (float(lprobs[index]) + lprob, sentence + [index], new_state) \n",
        "                        options.append(option)\n",
        "                else: # keep sentences ending in '</s>' as candidates\n",
        "                    options.append((lprob, sentence, current_state))\n",
        "            options.sort(key = lambda x: x[0], reverse=True) # sort by lprob\n",
        "            best_options = options[:k] # place top candidates in beam\n",
        "        best_options.sort(key = lambda x: x[0], reverse=True)\n",
        "        return best_options\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lY2PVZZjBf6t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "\n",
        "The section above was a bit complicated, but we're almost done. We just have to set up our training and validation loops. "
      ]
    },
    {
      "metadata": {
        "id": "14-6X0jEE0PW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class AverageMeter(object):\n",
        "  '''A handy class for moving averages''' \n",
        "  def __init__(self):\n",
        "    self.reset()\n",
        "  def reset(self):\n",
        "    self.val, self.avg, self.sum, self.count = 0, 0, 0, 0\n",
        "  def update(self, val, n=1):\n",
        "    self.val = val\n",
        "    self.sum += val * n\n",
        "    self.count += n\n",
        "    self.avg = self.sum / self.count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "afxP8M_gS4HI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "At each iteration of training, we update our model weights with gradient descent."
      ]
    },
    {
      "metadata": {
        "id": "fc_JGveeBfG_",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def train(train_iter, val_iter, model, criterion, optimizer, num_epochs):  \n",
        "    for epoch in range(num_epochs):\n",
        "      \n",
        "        # Validate model\n",
        "        with torch.no_grad():\n",
        "          val_loss = validate(val_iter, model, criterion) \n",
        "          print('Validating Epoch [{e}/{num_e}]\\t Average loss: {l:.3f}\\t Perplexity: {p:.3f}'.format(\n",
        "            e=epoch, num_e=num_epochs, l=val_loss, p=torch.FloatTensor([val_loss]).exp().item()))\n",
        "\n",
        "        # Train model\n",
        "        model.train()\n",
        "        losses = AverageMeter()\n",
        "        for i, batch in enumerate(train_iter): \n",
        "            src = batch.src.cuda() if use_gpu else batch.src\n",
        "            tgt = batch.trg.cuda() if use_gpu else batch.trg\n",
        "            \n",
        "            # Forward, backprop, optimizer\n",
        "            model.zero_grad()\n",
        "            scores = model(src, tgt)\n",
        "\n",
        "            # Remove <s> from target and </s> from scores (output)\n",
        "            scores = scores[:-1]\n",
        "            tgt = tgt[1:]           \n",
        "\n",
        "            # Reshape for loss function\n",
        "            scores = scores.view(scores.size(0) * scores.size(1), scores.size(2))\n",
        "            tgt = tgt.view(scores.size(0))\n",
        "\n",
        "            # Pass through loss function\n",
        "            loss = criterion(scores, tgt) \n",
        "            loss.backward()\n",
        "            losses.update(loss.item())\n",
        "\n",
        "            # Clip gradient norms and step optimizer\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            # Log within epoch\n",
        "            if i % 1000 == 10:\n",
        "                print('''Epoch [{e}/{num_e}]\\t Batch [{b}/{num_b}]\\t Loss: {l:.3f}'''.format(e=epoch+1, num_e=num_epochs, b=i, num_b=len(train_iter), l=losses.avg))\n",
        "\n",
        "        # Log after each epoch\n",
        "        print('''Epoch [{e}/{num_e}] complete. Loss: {l:.3f}'''.format(e=epoch+1, num_e=num_epochs, l=losses.avg))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bjZOJgd7TELE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "During training, we simply calculate the log likelihood of the ground truth sentence under our model. The exponential of this value is known as *perplexity*. If you are interested, there are more details on [Wikipedia](https://en.wikipedia.org/wiki/Perplexity). "
      ]
    },
    {
      "metadata": {
        "id": "1s7Dk9564sUr",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def validate(val_iter, model, criterion):\n",
        "    '''Calculate losses by teacher forcing on the validation set'''\n",
        "    model.eval()\n",
        "    losses = AverageMeter()\n",
        "    for i, batch in enumerate(val_iter):\n",
        "        src = batch.src.cuda() if use_gpu else batch.src\n",
        "        tgt = batch.trg.cuda() if use_gpu else batch.trg\n",
        "        \n",
        "        # Forward \n",
        "        scores = model(src, tgt)\n",
        "        scores = scores[:-1]\n",
        "        tgt = tgt[1:]           \n",
        "        \n",
        "        # Reshape for loss function\n",
        "        scores = scores.view(scores.size(0) * scores.size(1), scores.size(2))\n",
        "        tgt = tgt.view(scores.size(0))\n",
        "        num_words = (tgt != 0).float().sum()\n",
        "        \n",
        "        # Calculate loss\n",
        "        loss = criterion(scores, tgt) \n",
        "        losses.update(loss.item())\n",
        "    \n",
        "    return losses.avg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ipP3r03TTdeQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finally, we need an actual predict function, to see our translations!"
      ]
    },
    {
      "metadata": {
        "id": "uGgeAIPGEMVp",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def predict_from_text(model, input_sentence, SRC, TGT):\n",
        "    sent_german = input_sentence.split(' ') # sentence --> list of words\n",
        "    sent_indices = [SRC.vocab.stoi[word] if word in SRC.vocab.stoi else SRC.vocab.stoi['<unk>'] for word in sent_german]\n",
        "    sent = torch.LongTensor([sent_indices])\n",
        "    if use_gpu: sent = sent.cuda()\n",
        "    sent = sent.view(-1,1) # reshape to sl x bs\n",
        "    print('German: ' + ' '.join([SRC.vocab.itos[index] for index in sent_indices]))\n",
        "    # Predict five sentences with beam search \n",
        "    pred = model.predict(sent, beam_size=5) # returns list of 5 lists of word indices\n",
        "    out = ' '.join([TGT.vocab.itos[index] for index in pred[1:-1]])\n",
        "    print('English: ' + out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4444sq_mUbam",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Translation"
      ]
    },
    {
      "metadata": {
        "id": "FVlUZTb2Jyjg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It's time to run our model. First we load our embeddings. "
      ]
    },
    {
      "metadata": {
        "id": "1PU0wtkPRasL",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Load embeddings\n",
        "embedding_src, embedding_tgt = load_embeddings(SRC, TGT, 'emb-13353-de.npy', 'emb-11560-en.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DqIVcBVtTqaJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Then we create our model and move it onto the GPU. "
      ]
    },
    {
      "metadata": {
        "id": "7JacYrR2Fnwt",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Create model \n",
        "tokens = [TGT.vocab.stoi[x] for x in ['<s>', '</s>', '<pad>', '<unk>']]\n",
        "model = Seq2seq(embedding_src, embedding_tgt, 300, 2, 0.3, True, tokens_bos_eos_pad_unk=tokens)\n",
        "model = model.cuda() if use_gpu else model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bREio0seUxzr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we have our loss function (criterion) and optimizer. For our optimizer, we'll use Adam, but you can definitely use other loss functions (SGD, RMSProp, Adamax, etc.) as well. "
      ]
    },
    {
      "metadata": {
        "id": "kT4EGSBNGGUu",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Create weight to mask padding tokens for loss function\n",
        "weight = torch.ones(len(TGT.vocab))\n",
        "weight[TGT.vocab.stoi['<pad>']] = 0\n",
        "weight = weight.cuda() if use_gpu else weight"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "INe-LU06GGJr",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Create loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss(weight=weight)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=2e-3) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FWJQ7NZrVEAy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finally, we can train our model!"
      ]
    },
    {
      "metadata": {
        "id": "f6qdcw09GF-I",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 942
        },
        "collapsed": true,
        "outputId": "376ba5de-1581-42f0-9e5b-0b46adafd2a6",
        "executionInfo": {
          "status": "error",
          "timestamp": 1527187425723,
          "user_tz": 240,
          "elapsed": 1352584,
          "user": {
            "displayName": "Luke Melas-Kyriazi",
            "photoUrl": "//lh4.googleusercontent.com/-sqV5tJyQuHE/AAAAAAAAAAI/AAAAAAAANzo/VjDqA2ilBo4/s50-c-k-no/photo.jpg",
            "userId": "103761958259602667321"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "train(train_iter, val_iter, model, criterion, optimizer, 50)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validating Epoch [0/50]\t Average loss: 3.439\t Perplexity: 31.152\n",
            "Epoch [1/50]\t Batch [10/7443]\t Loss: 3.980\n",
            "Epoch [1/50]\t Batch [1010/7443]\t Loss: 3.609\n",
            "Epoch [1/50]\t Batch [2010/7443]\t Loss: 3.446\n",
            "Epoch [1/50]\t Batch [3010/7443]\t Loss: 3.336\n",
            "Epoch [1/50]\t Batch [4010/7443]\t Loss: 3.252\n",
            "Epoch [1/50]\t Batch [5010/7443]\t Loss: 3.182\n",
            "Epoch [1/50]\t Batch [6010/7443]\t Loss: 3.122\n",
            "Epoch [1/50]\t Batch [7010/7443]\t Loss: 3.073\n",
            "Epoch [1/50] complete. Loss: 3.052\n",
            "Validating Epoch [1/50]\t Average loss: 2.274\t Perplexity: 9.716\n",
            "Epoch [2/50]\t Batch [10/7443]\t Loss: 2.552\n",
            "Epoch [2/50]\t Batch [1010/7443]\t Loss: 2.550\n",
            "Epoch [2/50]\t Batch [2010/7443]\t Loss: 2.556\n",
            "Epoch [2/50]\t Batch [3010/7443]\t Loss: 2.548\n",
            "Epoch [2/50]\t Batch [4010/7443]\t Loss: 2.542\n",
            "Epoch [2/50]\t Batch [5010/7443]\t Loss: 2.538\n",
            "Epoch [2/50]\t Batch [6010/7443]\t Loss: 2.537\n",
            "Epoch [2/50]\t Batch [7010/7443]\t Loss: 2.533\n",
            "Epoch [2/50] complete. Loss: 2.531\n",
            "Validating Epoch [2/50]\t Average loss: 2.163\t Perplexity: 8.701\n",
            "Epoch [3/50]\t Batch [10/7443]\t Loss: 2.337\n",
            "Epoch [3/50]\t Batch [1010/7443]\t Loss: 2.319\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-0c793999c16c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-59-9a5864a40aa3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_iter, val_iter, model, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m# Pass through loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Beu5kTerXlny",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Training takes quite a while, so I've included a pretrained model at the link below."
      ]
    },
    {
      "metadata": {
        "id": "OqKUXvS0XvR7",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "collapsed": true,
        "outputId": "1260e754-47a5-4826-bdc4-fdc0bad4c3b1",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527188169675,
          "user_tz": 240,
          "elapsed": 4591,
          "user": {
            "displayName": "Luke Melas-Kyriazi",
            "photoUrl": "//lh4.googleusercontent.com/-sqV5tJyQuHE/AAAAAAAAAAI/AAAAAAAANzo/VjDqA2ilBo4/s50-c-k-no/photo.jpg",
            "userId": "103761958259602667321"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://www.dropbox.com/s/qu18vt3jisplchd/model.pkl"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-05-24 18:56:07--  https://www.dropbox.com/s/qu18vt3jisplchd/model.pkl?dl=0\r\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.9.1, 2620:100:601f:1::a27d:901\r\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.9.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://dl.dropboxusercontent.com/content_link/P4Mj4FOipfkT5FqoI8Qo7wbVtUQ9pZ4AYDZDajHa0IZWyFoKJqskR87HSX5J8Dta/file [following]\n",
            "--2018-05-24 18:56:07--  https://dl.dropboxusercontent.com/content_link/P4Mj4FOipfkT5FqoI8Qo7wbVtUQ9pZ4AYDZDajHa0IZWyFoKJqskR87HSX5J8Dta/file\n",
            "Resolving dl.dropboxusercontent.com (dl.dropboxusercontent.com)... 162.125.9.6, 2620:100:601f:6::a27d:906\n",
            "Connecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|162.125.9.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 56664376 (54M) [application/octet-stream]\n",
            "Saving to: ‚Äòmodel.pkl?dl=0‚Äô\n",
            "\n",
            "model.pkl?dl=0      100%[===================>]  54.04M  44.6MB/s    in 1.2s    \n",
            "\n",
            "2018-05-24 18:56:10 (44.6 MB/s) - ‚Äòmodel.pkl?dl=0‚Äô saved [56664376/56664376]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vVmld246XxBb",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('model.pkl'))\n",
        "model = model.cuda() if use_gpu else model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cxmAv9iFg1qz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can validate that the pretrained model words:"
      ]
    },
    {
      "metadata": {
        "id": "ksarzeeFg8xG",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f621a159-2377-4486-9eac-2db4edc8f726",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527188332824,
          "user_tz": 240,
          "elapsed": 6246,
          "user": {
            "displayName": "Luke Melas-Kyriazi",
            "photoUrl": "//lh4.googleusercontent.com/-sqV5tJyQuHE/AAAAAAAAAAI/AAAAAAAANzo/VjDqA2ilBo4/s50-c-k-no/photo.jpg",
            "userId": "103761958259602667321"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  val_loss = validate(val_iter, model, criterion) \n",
        "  print('Average loss: {l:.3f}\\t Perplexity: {p:.3f}'.format(l=val_loss, p=torch.FloatTensor([val_loss]).exp().item()))"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average loss: 1.865\t Perplexity: 6.459\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-UW79h9ahMC6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finally, we can translate some German sentences! Let's try a few from the German newspaper *S√ºddeutsche Zeitung*."
      ]
    },
    {
      "metadata": {
        "id": "84sqEuVlhQij",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f9fe97d3-7c6e-4f1f-cb04-8fde0d3810d6",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527189579316,
          "user_tz": 240,
          "elapsed": 1328,
          "user": {
            "displayName": "Luke Melas-Kyriazi",
            "photoUrl": "//lh4.googleusercontent.com/-sqV5tJyQuHE/AAAAAAAAAAI/AAAAAAAANzo/VjDqA2ilBo4/s50-c-k-no/photo.jpg",
            "userId": "103761958259602667321"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "input = \"Ich kenne nur Berge, ich bleibe in den Bergen und ich liebe die Berge .\"\n",
        "predict_from_text(model, input, SRC, TGT)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "German: Ich kenne nur <unk> ich bleibe in den Bergen und ich liebe die Berge .\n",
            "English: I only know I 'm staying in the hills , and I love the mountains .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0gvxegyzl5LM",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "89cdca50-0218-4c92-f89c-65a1038ff643",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527189581098,
          "user_tz": 240,
          "elapsed": 458,
          "user": {
            "displayName": "Luke Melas-Kyriazi",
            "photoUrl": "//lh4.googleusercontent.com/-sqV5tJyQuHE/AAAAAAAAAAI/AAAAAAAANzo/VjDqA2ilBo4/s50-c-k-no/photo.jpg",
            "userId": "103761958259602667321"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "input = \"Ihre Bergung erwies sich als komplizierter als gedacht .\" \n",
        "predict_from_text(model, input, SRC, TGT)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "German: Ihre <unk> erwies sich als komplizierter als gedacht .\n",
            "English: Her <unk> turned out to be more complicated than that .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mZVqnazI7MuH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Conclusion\n",
        "\n",
        "In this post, we built a neural machine translation system in PyTorch. With just a few more additions, a library full of training data, and a warehouse full of computing power, you'll have your very own Google Translate!\n",
        "\n",
        "If you found these results exciting, there are lots of exciting extensions: \n",
        "\n",
        "* Unsupervise machine translation -- translating without paired training data: [Lample et al.](https://arxiv.org/abs/1804.07755)\n",
        "* Multilingual machine translation -- translating between lots of languages: [Johnson et al.](https://arxiv.org/abs/1611.04558)\n",
        "* The details of Google's system: [Wu et al.](https://arxiv.org/abs/1609.08144)\n",
        "\n",
        "I hope you enjoyed the dive into machine translation. If you would like to see anything else on the blog in the future, reach out to me! "
      ]
    }
  ]
}
